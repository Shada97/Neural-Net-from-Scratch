# Neural-Net-from-Scratch
I built a basic neural network from scratch, featuring one hidden layer, to train on the MNIST dataset without relying on deep learning frameworks like TensorFlow or PyTorch. The network achieves an accuracy of 90% on the training set and 91% on the test set.

The primary objective was to gain a deep understanding of the underlying mathematics behind neural networks and how they function. To this end, I implemented several activation functions—ReLU, Leaky ReLU, Softmax, Sigmoid, and Tanh—along with their derivatives, using only NumPy.

Additionally, I utilized Matplotlib to visualize some test set samples for demonstration
